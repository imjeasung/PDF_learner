# AI ì§ˆì˜ì‘ë‹µ ëª¨ë“ˆ (RAG ë°©ì‹)
# AI Provider Managerë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

import os
import json
import re
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv

# AI Provider Manager ì„í¬íŠ¸
try:
    from ai_providers import get_ai_manager
    from config import settings
    USE_AI_MANAGER = True
except ImportError:
    # ê¸°ì¡´ OpenAI ë°©ì‹ìœ¼ë¡œ í´ë°±
    import openai
    USE_AI_MANAGER = False

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class ChatBot:
    """PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ì±—ë´‡ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    
    def __init__(self, data_folder: str = "data"):
        """
        ì±—ë´‡ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            data_folder: ë°ì´í„°ê°€ ì €ì¥ëœ í´ë”
        """
        self.data_folder = data_folder
        self.vector_db_path = os.getenv("VECTOR_DB_PATH", f"{data_folder}/vector_db")
        
        # í•„ìš”í•œ í´ë” ìƒì„±
        Path(self.vector_db_path).mkdir(parents=True, exist_ok=True)
        
        # AI Provider Manager ì´ˆê¸°í™”
        if USE_AI_MANAGER:
            try:
                self.ai_manager = get_ai_manager()
                openai_config = settings.AI_PROVIDERS_CONFIG.get("openai", {})
                self.model = openai_config.get("default_model", "gpt-3.5-turbo")
                self.embedding_model = openai_config.get("embedding_model", "text-embedding-ada-002")
                self.max_tokens = int(os.getenv("MAX_TOKENS", "1000"))
                self.temperature = float(os.getenv("TEMPERATURE", "0.7"))
                print(f"ğŸ¤– ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ (AI Manager ì‚¬ìš©, ëª¨ë¸: {self.model})")
            except Exception as e:
                print(f"âš ï¸ AI Manager ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±: {str(e)}")
                self._init_legacy_openai()
        else:
            self._init_legacy_openai()
        
        # ChromaDB ì„¤ì •
        self.chroma_client = chromadb.PersistentClient(
            path=self.vector_db_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # í™œì„±í™”ëœ ë¬¸ì„œë“¤ì˜ ì»¬ë ‰ì…˜ ì €ì¥
        self.active_collections = {}
    
    def _init_legacy_openai(self):
        """ê¸°ì¡´ OpenAI ë°©ì‹ìœ¼ë¡œ ì´ˆê¸°í™” (í´ë°±ìš©)"""
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("âŒ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # Initialize OpenAI client
        self.client = openai.OpenAI(api_key=self.api_key)
        self.model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        self.embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-ada-002")
        self.max_tokens = int(os.getenv("MAX_TOKENS", "1000"))
        self.temperature = float(os.getenv("TEMPERATURE", "0.7"))
        self.ai_manager = None
        
        print(f"ğŸ¤– ì±—ë´‡ ì´ˆê¸°í™” ì™„ë£Œ (ë ˆê±°ì‹œ ëª¨ë“œ, ëª¨ë¸: {self.model})")
    
    def _sanitize_collection_name(self, file_name: str) -> str:
        """
        íŒŒì¼ëª…ì„ ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ ê·œì¹™ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            file_name: ì›ë³¸ íŒŒì¼ëª…
            
        Returns:
            ì•ˆì „í•œ ì»¬ë ‰ì…˜ ì´ë¦„
        """
        # .pdf í™•ì¥ì ì œê±°
        clean_name = file_name.replace('.pdf', '')
        
        # í•œê¸€ê³¼ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì˜ë¬¸ê³¼ ìˆ«ìë¡œ ë³€í™˜
        # 1. ì˜ë¬¸, ìˆ«ì, ì¼ë¶€ í—ˆìš©ëœ íŠ¹ìˆ˜ë¬¸ìë§Œ ë‚¨ê¸°ê¸°
        safe_chars = re.sub(r'[^a-zA-Z0-9._-]', '_', clean_name)
        
        # 2. ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        safe_chars = re.sub(r'_+', '_', safe_chars)
        
        # 3. ì‹œì‘ê³¼ ëì˜ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        safe_chars = safe_chars.strip('_')
        
        # 4. ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ í•´ì‹œê°’ ì‚¬ìš©
        if len(safe_chars) < 3:
            # ì›ë³¸ íŒŒì¼ëª…ì˜ í•´ì‹œê°’ ìƒì„±
            hash_value = hashlib.md5(file_name.encode('utf-8')).hexdigest()[:8]
            safe_chars = f"doc_{hash_value}"
        else:
            # doc_ ì ‘ë‘ì‚¬ ì¶”ê°€
            safe_chars = f"doc_{safe_chars}"
        
        # 5. ê¸¸ì´ ì œí•œ (ChromaDBëŠ” 512ìê¹Œì§€ í—ˆìš©í•˜ì§€ë§Œ ì ë‹¹íˆ ì œí•œ)
        if len(safe_chars) > 100:
            # ì›ë³¸ íŒŒì¼ëª…ì˜ í•´ì‹œê°’ìœ¼ë¡œ ë‹¨ì¶•
            hash_value = hashlib.md5(file_name.encode('utf-8')).hexdigest()[:8]
            safe_chars = f"doc_{safe_chars[:50]}_{hash_value}"
        
        # 6. ìµœì¢… ê²€ì¦: ì˜ë¬¸/ìˆ«ìë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ”ì§€ í™•ì¸
        if not re.match(r'^[a-zA-Z0-9].*[a-zA-Z0-9]$', safe_chars):
            hash_value = hashlib.md5(file_name.encode('utf-8')).hexdigest()[:8]
            safe_chars = f"doc_{hash_value}"
        
        return safe_chars
    
    def create_vector_database(self, extracted_data: Dict) -> str:
        """
        ì¶”ì¶œëœ PDF ë°ì´í„°ë¡œë¶€í„° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            extracted_data: PDFì—ì„œ ì¶”ì¶œëœ ë°ì´í„°
            
        Returns:
            ìƒì„±ëœ ì»¬ë ‰ì…˜ ì´ë¦„
        """
        try:
            file_name = extracted_data["file_name"]
            collection_name = self._sanitize_collection_name(file_name)
            
            print(f"ğŸ“Š ë²¡í„° DB ìƒì„± ì‹œì‘: {file_name}")
            print(f"  ğŸ·ï¸ ì»¬ë ‰ì…˜ ì´ë¦„: {collection_name}")
            
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
            try:
                self.chroma_client.delete_collection(name=collection_name)
            except:
                pass  # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
            
            # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            collection = self.chroma_client.create_collection(
                name=collection_name,
                metadata={"description": f"Vector database for {file_name}"}
            )
            
            # í…ìŠ¤íŠ¸ ì²­í¬ë“¤ ì¤€ë¹„
            documents = []
            metadatas = []
            ids = []
            
            # í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            for page in extracted_data["pages"]:
                page_num = page["page_number"]
                page_text = page["text"].strip()
                
                if page_text:
                    # í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
                    chunks = self._split_page_text(page_text)
                    
                    for i, chunk in enumerate(chunks):
                        if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
                            doc_id = f"{file_name}_page{page_num}_chunk{i}"
                            
                            documents.append(chunk)
                            metadatas.append({
                                "file_name": file_name,
                                "page_number": page_num,
                                "chunk_index": i,
                                "source": f"í˜ì´ì§€ {page_num}"
                            })
                            ids.append(doc_id)
            
            print(f"  ğŸ“ ì´ {len(documents)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±")
            
            # ì„ë² ë”© ìƒì„± ë° ì €ì¥ (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬)
            batch_size = int(os.getenv("EMBEDDING_BATCH_SIZE", "50"))
            for i in range(0, len(documents), batch_size):
                end_idx = min(i + batch_size, len(documents))
                batch_docs = documents[i:end_idx]
                batch_metas = metadatas[i:end_idx]
                batch_ids = ids[i:end_idx]
                
                # OpenAI ì„ë² ë”© ìƒì„±
                embeddings = self._generate_embeddings(batch_docs)
                
                # ChromaDBì— ì €ì¥
                collection.add(
                    documents=batch_docs,
                    metadatas=batch_metas,
                    ids=batch_ids,
                    embeddings=embeddings
                )
                
                print(f"  ğŸ’¾ ë°°ì¹˜ {i//batch_size + 1} ì €ì¥ ì™„ë£Œ ({end_idx}/{len(documents)})")
            
            # ì»¬ë ‰ì…˜ì„ í™œì„±í™” ëª©ë¡ì— ì¶”ê°€
            self.active_collections[file_name] = collection_name
            
            print(f"ğŸ‰ ë²¡í„° DB ìƒì„± ì™„ë£Œ: {collection_name}")
            return collection_name
            
        except Exception as e:
            print(f"âŒ ë²¡í„° DB ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë²¡í„° DB ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def _split_page_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        chunks = []
        sentences = text.split('. ')
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
            else:
                current_chunk += sentence + ". "
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # AI Manager ì‚¬ìš© ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ í´ë°±
            if self.ai_manager:
                embedding_result = self.ai_manager.generate_embeddings(
                    texts=texts,
                    model=self.embedding_model
                )
                # EmbeddingResult ê°ì²´ì—ì„œ ì‹¤ì œ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                return embedding_result.embeddings
            else:
                response = self.client.embeddings.create(
                    model=self.embedding_model,
                    input=texts
                )
                
                embeddings = [item.embedding for item in response.data]
                return embeddings
            
        except Exception as e:
            print(f"  âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì„ì‹œë¡œ ë¹ˆ ì„ë² ë”© ë°˜í™˜
            embedding_dim = int(os.getenv("EMBEDDING_DIMENSION", "1536"))  # ada-002ì˜ ê¸°ë³¸ ì°¨ì›
            return [[0.0] * embedding_dim for _ in texts]
    
    def answer_question(self, question: str, file_name: str = None, top_k: int = 3) -> Dict:
        """
        ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ PDF ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            file_name: ê²€ìƒ‰í•  ë¬¸ì„œëª… (Noneì´ë©´ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰)
            top_k: ê²€ìƒ‰í•  ê´€ë ¨ ë¬¸ì„œ ê°œìˆ˜
            
        Returns:
            ë‹µë³€ê³¼ ì¶œì²˜ ì •ë³´
        """
        try:
            print(f"â“ ì§ˆë¬¸ ì²˜ë¦¬: {question}")
            
            # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_chunks = self._search_relevant_chunks(question, file_name, top_k)
            
            if not relevant_chunks:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì—…ë¡œë“œëœ PDF ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "confidence": 0.0
                }
            
            # AIë¡œ ë‹µë³€ ìƒì„±
            answer_data = self._generate_answer_with_ai(question, relevant_chunks)
            
            print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer_data
            
        except Exception as e:
            print(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return {
                "answer": f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "sources": [],
                "confidence": 0.0
            }
    
    def _search_relevant_chunks(self, question: str, file_name: str = None, top_k: int = 3) -> List[Dict]:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        try:
            # ì§ˆë¬¸ì˜ ì„ë² ë”© ìƒì„±
            question_embedding = self._generate_embeddings([question])[0]
            
            relevant_chunks = []
            
            # íŠ¹ì • ë¬¸ì„œê°€ ì§€ì •ëœ ê²½ìš°
            if file_name and file_name in self.active_collections:
                collection_name = self.active_collections[file_name]
                collection = self.chroma_client.get_collection(name=collection_name)
                
                results = collection.query(
                    query_embeddings=[question_embedding],
                    n_results=top_k
                )
                
                relevant_chunks.extend(self._format_search_results(results))
            
            # ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰
            else:
                for doc_name, collection_name in self.active_collections.items():
                    try:
                        collection = self.chroma_client.get_collection(name=collection_name)
                        
                        results = collection.query(
                            query_embeddings=[question_embedding],
                            n_results=max(1, top_k // len(self.active_collections))
                        )
                        
                        relevant_chunks.extend(self._format_search_results(results))
                    except Exception as e:
                        print(f"  âš ï¸ ì»¬ë ‰ì…˜ {collection_name} ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                        continue
            
            # ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
            relevant_chunks.sort(key=lambda x: x.get("distance", 1.0))
            return relevant_chunks[:top_k]
            
        except Exception as e:
            print(f"  âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _format_search_results(self, results) -> List[Dict]:
        """ChromaDB ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        formatted = []
        
        if results["documents"] and len(results["documents"]) > 0:
            documents = results["documents"][0]
            metadatas = results["metadatas"][0] if results["metadatas"] else []
            distances = results["distances"][0] if results["distances"] else []
            
            for i, doc in enumerate(documents):
                metadata = metadatas[i] if i < len(metadatas) else {}
                distance = distances[i] if i < len(distances) else 1.0
                
                formatted.append({
                    "content": doc,
                    "file_name": metadata.get("file_name", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "page_number": metadata.get("page_number", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "source": metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "distance": distance,
                    "similarity": 1.0 - distance  # ìœ ì‚¬ë„ ê³„ì‚°
                })
        
        return formatted
    
    def _generate_answer_with_ai(self, question: str, relevant_chunks: List[Dict]) -> Dict:
        """AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context_parts = []
            sources = []
            
            for i, chunk in enumerate(relevant_chunks):
                context_parts.append(f"[ì¶œì²˜ {i+1}: {chunk['source']}]\n{chunk['content']}")
                sources.append({
                    "file_name": chunk["file_name"],
                    "page_number": chunk["page_number"],
                    "source": chunk["source"],
                    "similarity": round(chunk.get("similarity", 0.0), 3)
                })
            
            context = "\n\n".join(context_parts)
            
            # AI í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”
3. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. ë„ì›€ì´ ë˜ëŠ” ì¶”ê°€ ì„¤ëª…ì„ í¬í•¨í•˜ì„¸ìš”

ë‹µë³€:
"""

            # AI Manager ì‚¬ìš© ë˜ëŠ” ê¸°ì¡´ ë°©ì‹ í´ë°±
            if self.ai_manager:
                generation_result = self.ai_manager.generate_text(
                    prompt=prompt,
                    model=self.model,
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                # GenerationResult ê°ì²´ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                answer = generation_result.text
            else:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=self.max_tokens,
                    temperature=self.temperature
                )
                
                answer = response.choices[0].message.content.strip()
            
            # ì‹ ë¢°ë„ ê³„ì‚° (ìœ ì‚¬ë„ ì ìˆ˜ë“¤ì˜ í‰ê· )
            if relevant_chunks:
                confidence = sum(chunk.get("similarity", 0.0) for chunk in relevant_chunks) / len(relevant_chunks)
            else:
                confidence = 0.0
            
            return {
                "answer": answer,
                "sources": sources,
                "confidence": round(confidence, 3),
                "context_used": len(relevant_chunks)
            }
            
        except Exception as e:
            print(f"  âš ï¸ AI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": sources if 'sources' in locals() else [],
                "confidence": 0.0
            }
    
    def get_available_documents(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return list(self.active_collections.keys())
    
    def remove_document(self, file_name: str) -> bool:
        """
        ë¬¸ì„œë¥¼ ì±—ë´‡ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            file_name: ì œê±°í•  ë¬¸ì„œëª… (í™•ì¥ì ì œì™¸)
            
        Returns:
            ì œê±° ì„±ê³µ ì—¬ë¶€
        """
        try:
            # í™œì„± ì»¬ë ‰ì…˜ì—ì„œ ì œê±°
            if file_name in self.active_collections:
                collection_name = self.active_collections[file_name]
                
                try:
                    # ChromaDBì—ì„œ ì»¬ë ‰ì…˜ ì‚­ì œ
                    self.chroma_client.delete_collection(name=collection_name)
                    print(f"ğŸ—‘ï¸ ChromaDB ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
                except Exception as e:
                    print(f"âš ï¸ ChromaDB ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
                
                # ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                del self.active_collections[file_name]
                print(f"ğŸ§¹ ë¬¸ì„œ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°: {file_name}")
                return True
            else:
                print(f"âš ï¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_name}")
                return False
                
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì œê±° ì‹¤íŒ¨: {str(e)}")
            return False
    
    def load_document_for_chat(self, file_name: str) -> bool:
        """
        ì±„íŒ…ì„ ìœ„í•´ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            file_name: ë¡œë“œí•  ë¬¸ì„œëª…
            
        Returns:
            ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì¶”ì¶œëœ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
            extracted_file = f"{self.data_folder}/extracted/{file_name}_extracted.json"
            
            if not os.path.exists(extracted_file):
                print(f"âŒ ì¶”ì¶œëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {extracted_file}")
                return False
            
            # ë°ì´í„° ë¡œë“œ
            with open(extracted_file, 'r', encoding='utf-8') as f:
                extracted_data = json.load(f)
            
            # ë²¡í„° DB ìƒì„±
            self.create_vector_database(extracted_data)
            return True
            
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return False

# í¸ì˜ í•¨ìˆ˜ë“¤
def create_chatbot() -> ChatBot:
    """ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return ChatBot()

def setup_document_for_chat(file_name: str) -> bool:
    """ë¬¸ì„œë¥¼ ì±„íŒ…ìš©ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
    chatbot = create_chatbot()
    return chatbot.load_document_for_chat(file_name)

# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    import sys
    import io
    
    # Windows í™˜ê²½ì—ì„œ UTF-8 ì¶œë ¥ ì„¤ì •
    if sys.platform == "win32":
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    print("ğŸ§ª ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ì±—ë´‡ ìƒì„±
        chatbot = ChatBot()
        print("âœ… ì±—ë´‡ ì´ˆê¸°í™” ì„±ê³µ")
        
        print("ğŸ‰ ì±—ë´‡ ì´ˆê¸°í™” ì„±ê³µ!")
        print("ğŸ’¡ ì´ ëª¨ë“ˆì€ main.pyì—ì„œ importí•˜ì—¬ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ì±—ë´‡ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print("ğŸ’¡ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")